{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35653ecd-4740-41e8-82ee-9be3647f3a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf24d57e-7173-4aa3-8d34-a6d474d185bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"CUDA_VISIBLE_DEVICES =\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a45041b4-ff46-476d-8963-7e0922589abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  7 09:32:41 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:01:01.0 Off |                    0 |\n",
      "| N/A   53C    P0             79W /  300W |   58883MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off |   00000000:01:02.0 Off |                    0 |\n",
      "| N/A   76C    P0            300W /  300W |   57227MiB /  81920MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1227776      C   ...844/miniconda3/envs/doge/bin/python        522MiB |\n",
      "|    0   N/A  N/A   1227934      C   ...844/miniconda3/envs/doge/bin/python        522MiB |\n",
      "|    0   N/A  N/A   1414209      C   VLLM::EngineCore                            57818MiB |\n",
      "|    1   N/A  N/A   1240731      C   python3                                     43300MiB |\n",
      "|    1   N/A  N/A   1410926      C   python                                      13912MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5d6001-68ab-47f4-8a18-9ed386ce26f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-07:09:14:06,974 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-12-07:09:14:06,974 INFO     [main2.py:78] Loading the tokenizer\n",
      "2025-12-07:09:14:07,448 INFO     [main2.py:80] Loaded the tokenizer\n",
      "2025-12-07:09:14:07,448 INFO     [main2.py:83] Loading the Datasets\n",
      "2025-12-07:09:14:16,524 INFO     [main2.py:91] Loaded the Datasets\n",
      "2025-12-07:09:14:16,524 INFO     [main2.py:92] Sample from training dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-12-07:09:14:16,524 INFO     [main2.py:95] Sample from validation dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-12-07:09:14:16,524 INFO     [main2.py:99] Tokenizing the Datasets\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python 2SSP/main2.py --model=meta-llama/Llama-2-7b-hf --dense --evaluate_perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b41dc6-7097-4649-b2a1-cfc7ff6678a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 2SSP/main2.py --model=mistralai/Mistral-7B-v0.3 --dense --evaluate_perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632534d3-8f2e-40af-8e4d-52e04521b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python 2SSP/main2.py --model=Qwen/Qwen2.5-7B --dense --evaluate_perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7efff8b7-3033-4cb5-ad84-7bc5b725e969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-23:21:17:44,659 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-10-23:21:17:44,659 INFO     [main2.py:78] Loading the tokenizer\n",
      "2025-10-23:21:17:45,062 INFO     [main2.py:80] Loaded the tokenizer\n",
      "2025-10-23:21:17:45,062 INFO     [main2.py:83] Loading the Datasets\n",
      "2025-10-23:21:17:54,369 INFO     [main2.py:91] Loaded the Datasets\n",
      "2025-10-23:21:17:54,369 INFO     [main2.py:92] Sample from training dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-10-23:21:17:54,369 INFO     [main2.py:95] Sample from validation dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-10-23:21:17:54,370 INFO     [main2.py:99] Tokenizing the Datasets\n",
      "2025-10-23:21:20:33,068 INFO     [main2.py:103] Tokenized the datasets\n",
      "2025-10-23:21:23:03,682 INFO     [main2.py:149] Loading the model\n",
      "2025-10-23:21:23:03,682 INFO     [utilities.py:33] Loading the model\n",
      "2025-10-23:21:23:04,133 INFO     [modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.12s/it]\n",
      "2025-10-23:21:23:10,657 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-10-23:21:23:10,658 INFO     [main2.py:185] Pruning rate 25.0 (equivalent of 8.0 blocks)\n",
      "2025-10-23:21:23:10,659 INFO     [pruning.py:248] Pruning 5 attention submodules\n",
      "First stage: 100%|██████████████████████████████| 32/32 [00:48<00:00,  1.51s/it]\n",
      "Second stage: 100%|███████████████████████████████| 5/5 [00:21<00:00,  4.26s/it]\n",
      "2025-10-23:21:24:20,784 INFO     [main2.py:206] Pruning Time: 70.12597608566284 s\n",
      "2025-10-23:21:24:20,785 INFO     [utilities.py:28] [Pruned model] Full number of parameters = 5119414272\n",
      "2025-10-23:21:24:20,785 INFO     [utilities.py:29] [Pruned model] Main model number of parameters = 4857266176\n",
      "Calculating perplexity: 100%|█████████████████| 166/166 [00:22<00:00,  7.26it/s]\n",
      "2025-10-23:21:24:43,768 INFO     [main2.py:224] Perplexity (wikitext2): 10.25\n",
      "2025-10-23:21:24:43,913 INFO     [utilities.py:33] Loading the model\n",
      "2025-10-23:21:24:44,640 INFO     [modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.78s/it]\n",
      "2025-10-23:21:24:50,498 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-10-23:21:24:50,498 INFO     [main2.py:185] Pruning rate 37.5 (equivalent of 12.0 blocks)\n",
      "2025-10-23:21:24:50,500 INFO     [pruning.py:248] Pruning 9 attention submodules\n",
      "First stage: 100%|██████████████████████████████| 32/32 [00:48<00:00,  1.53s/it]\n",
      "Second stage: 100%|███████████████████████████████| 9/9 [00:32<00:00,  3.62s/it]\n",
      "2025-10-23:21:26:12,304 INFO     [main2.py:206] Pruning Time: 81.80592703819275 s\n",
      "2025-10-23:21:26:12,306 INFO     [utilities.py:28] [Pruned model] Full number of parameters = 4309913600\n",
      "2025-10-23:21:26:12,306 INFO     [utilities.py:29] [Pruned model] Main model number of parameters = 4047765504\n",
      "Calculating perplexity: 100%|█████████████████| 166/166 [00:19<00:00,  8.31it/s]\n",
      "2025-10-23:21:26:32,378 INFO     [main2.py:224] Perplexity (wikitext2): 19.125\n",
      "2025-10-23:21:26:32,515 INFO     [utilities.py:33] Loading the model\n",
      "2025-10-23:21:26:32,784 INFO     [modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.27s/it]\n",
      "2025-10-23:21:26:39,626 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-10-23:21:26:39,626 INFO     [main2.py:185] Pruning rate 50.0 (equivalent of 16.0 blocks)\n",
      "2025-10-23:21:26:39,627 INFO     [pruning.py:248] Pruning 13 attention submodules\n",
      "First stage: 100%|██████████████████████████████| 32/32 [00:48<00:00,  1.50s/it]\n",
      "Second stage: 100%|█████████████████████████████| 13/13 [00:38<00:00,  2.97s/it]\n",
      "2025-10-23:21:28:07,257 INFO     [main2.py:206] Pruning Time: 87.63164734840393 s\n",
      "2025-10-23:21:28:07,259 INFO     [utilities.py:28] [Pruned model] Full number of parameters = 3500412928\n",
      "2025-10-23:21:28:07,259 INFO     [utilities.py:29] [Pruned model] Main model number of parameters = 3238264832\n",
      "Calculating perplexity: 100%|█████████████████| 166/166 [00:16<00:00,  9.96it/s]\n",
      "2025-10-23:21:28:24,010 INFO     [main2.py:224] Perplexity (wikitext2): 41.75\n",
      "2025-10-23:21:28:24,147 INFO     [utilities.py:33] Loading the model\n",
      "2025-10-23:21:28:24,442 INFO     [modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.67s/it]\n"
     ]
    }
   ],
   "source": [
    "!python 2SSP/main2.py --model=meta-llama/Llama-2-7b-hf --pruning_method=2ssp --sparsity_rate=-2 --evaluate_perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f56d7c1-69d9-460f-8e39-bfb408c5d1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-23:21:38:14,621 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-10-23:21:38:14,621 INFO     [main2.py:78] Loading the tokenizer\n",
      "2025-10-23:21:38:15,012 INFO     [main2.py:80] Loaded the tokenizer\n",
      "2025-10-23:21:38:15,012 INFO     [main2.py:83] Loading the Datasets\n",
      "2025-10-23:21:38:23,950 INFO     [main2.py:91] Loaded the Datasets\n",
      "2025-10-23:21:38:23,950 INFO     [main2.py:92] Sample from training dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-10-23:21:38:23,950 INFO     [main2.py:95] Sample from validation dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-10-23:21:38:23,950 INFO     [main2.py:99] Tokenizing the Datasets\n",
      "2025-10-23:21:41:02,755 INFO     [main2.py:103] Tokenized the datasets\n",
      "2025-10-23:21:43:33,723 INFO     [main2.py:149] Loading the model\n",
      "2025-10-23:21:43:33,723 INFO     [utilities.py:33] Loading the model\n",
      "2025-10-23:21:43:34,150 INFO     [modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.36s/it]\n",
      "2025-10-23:21:43:41,575 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-10-23:21:43:41,575 INFO     [main2.py:185] Pruning rate 25.0 (equivalent of 8.0 blocks)\n",
      "2025-10-23:21:43:41,576 INFO     [pruning.py:248] Pruning 5 attention submodules\n",
      "First stage: 100%|██████████████████████████████| 32/32 [00:48<00:00,  1.52s/it]\n",
      "Second stage: 100%|███████████████████████████████| 5/5 [00:21<00:00,  4.25s/it]\n",
      "2025-10-23:21:44:51,798 INFO     [main2.py:206] Pruning Time: 70.22313642501831 s\n",
      "2025-10-23:21:44:51,799 INFO     [utilities.py:28] [Pruned model] Full number of parameters = 5119414272\n",
      "2025-10-23:21:44:51,800 INFO     [utilities.py:29] [Pruned model] Main model number of parameters = 4857266176\n",
      "Calculating perplexity: 100%|███████████████| 6270/6270 [14:45<00:00,  7.08it/s]\n",
      "2025-10-23:21:59:37,096 INFO     [main2.py:224] Perplexity (my_dataset): 3.078125\n",
      "2025-10-23:21:59:37,234 INFO     [utilities.py:33] Loading the model\n",
      "2025-10-23:21:59:37,521 INFO     [modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.98s/it]\n",
      "2025-10-23:21:59:43,798 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-10-23:21:59:43,798 INFO     [main2.py:185] Pruning rate 37.5 (equivalent of 12.0 blocks)\n",
      "2025-10-23:21:59:43,799 INFO     [pruning.py:248] Pruning 9 attention submodules\n",
      "First stage: 100%|██████████████████████████████| 32/32 [00:45<00:00,  1.42s/it]\n",
      "Second stage: 100%|███████████████████████████████| 9/9 [00:32<00:00,  3.65s/it]\n",
      "2025-10-23:22:01:02,439 INFO     [main2.py:206] Pruning Time: 78.64107966423035 s\n",
      "2025-10-23:22:01:02,440 INFO     [utilities.py:28] [Pruned model] Full number of parameters = 4309913600\n",
      "2025-10-23:22:01:02,440 INFO     [utilities.py:29] [Pruned model] Main model number of parameters = 4047765504\n",
      "Calculating perplexity: 100%|███████████████| 6270/6270 [12:48<00:00,  8.15it/s]\n",
      "2025-10-23:22:13:51,457 INFO     [main2.py:224] Perplexity (my_dataset): 3.390625\n",
      "2025-10-23:22:13:51,596 INFO     [utilities.py:33] Loading the model\n",
      "2025-10-23:22:13:51,881 INFO     [modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:06<00:00,  3.27s/it]\n",
      "2025-10-23:22:13:58,707 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-10-23:22:13:58,707 INFO     [main2.py:185] Pruning rate 50.0 (equivalent of 16.0 blocks)\n",
      "2025-10-23:22:13:58,709 INFO     [pruning.py:248] Pruning 13 attention submodules\n",
      "First stage: 100%|██████████████████████████████| 32/32 [00:48<00:00,  1.50s/it]\n",
      "Second stage: 100%|█████████████████████████████| 13/13 [00:38<00:00,  2.98s/it]\n",
      "2025-10-23:22:15:26,409 INFO     [main2.py:206] Pruning Time: 87.70205545425415 s\n",
      "2025-10-23:22:15:26,411 INFO     [utilities.py:28] [Pruned model] Full number of parameters = 3500412928\n",
      "2025-10-23:22:15:26,411 INFO     [utilities.py:29] [Pruned model] Main model number of parameters = 3238264832\n",
      "Calculating perplexity: 100%|███████████████| 6270/6270 [10:38<00:00,  9.81it/s]\n",
      "2025-10-23:22:26:05,503 INFO     [main2.py:224] Perplexity (my_dataset): 4.96875\n",
      "2025-10-23:22:26:05,641 INFO     [utilities.py:33] Loading the model\n",
      "2025-10-23:22:26:05,932 INFO     [modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "!python 2SSP/main2.py --model=meta-llama/Llama-2-7b-hf --pruning_method=2ssp --sparsity_rate=-2 --evaluate_perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf92d30-0d35-4dba-bfe3-4bd2fc04fe45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Torch)",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

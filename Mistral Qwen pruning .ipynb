{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a728e8fb-8f17-48e3-8c9e-27b4deb75d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-07:09:16:23,308 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-12-07:09:16:23,308 INFO     [main2.py:78] Loading the tokenizer\n",
      "2025-12-07:09:16:23,797 INFO     [main2.py:80] Loaded the tokenizer\n",
      "2025-12-07:09:16:23,797 INFO     [main2.py:83] Loading the Datasets\n",
      "2025-12-07:09:16:32,683 INFO     [main2.py:91] Loaded the Datasets\n",
      "2025-12-07:09:16:32,683 INFO     [main2.py:92] Sample from training dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-12-07:09:16:32,683 INFO     [main2.py:95] Sample from validation dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-12-07:09:16:32,683 INFO     [main2.py:99] Tokenizing the Datasets\n",
      "2025-12-07:09:19:12,912 INFO     [main2.py:103] Tokenized the datasets\n",
      "2025-12-07:09:21:46,471 INFO     [main2.py:118] Dense model evaluation\n",
      "2025-12-07:09:21:46,471 INFO     [main2.py:119] Loading the model\n",
      "2025-12-07:09:21:46,471 INFO     [utilities.py:33] Loading the model\n",
      "2025-12-07:09:21:47,119 INFO     [modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.55s/it]\n",
      "2025-12-07:09:21:58,560 INFO     [utilities.py:28] [Dense model] Full number of parameters = 6738415616\n",
      "2025-12-07:09:21:58,560 INFO     [utilities.py:29] [Dense model] Main model number of parameters = 6476267520\n",
      "Calculating perplexity: 100%|███████████████| 6270/6270 [39:03<00:00,  2.68it/s]\n",
      "2025-12-07:10:01:02,383 INFO     [main2.py:135] Perplexity (my_dataset): 2.71875\n"
     ]
    }
   ],
   "source": [
    "!python 2SSP/main2.py --model=meta-llama/Llama-2-7b-hf --dense --evaluate_perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44aa175-73d0-4ece-a223-f36d740a100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-07:10:01:07,567 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-12-07:10:01:07,568 INFO     [main2.py:78] Loading the tokenizer\n",
      "tokenizer_config.json: 137kB [00:00, 35.5MB/s]\n",
      "tokenizer.model: 100%|████████████████████████| 587k/587k [00:01<00:00, 417kB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 414/414 [00:00<00:00, 4.50MB/s]\n",
      "tokenizer.json: 1.96MB [00:00, 21.8MB/s]\n",
      "2025-12-07:10:01:12,336 INFO     [main2.py:80] Loaded the tokenizer\n",
      "2025-12-07:10:01:12,337 INFO     [main2.py:83] Loading the Datasets\n",
      "2025-12-07:10:01:21,363 INFO     [main2.py:91] Loaded the Datasets\n",
      "2025-12-07:10:01:21,363 INFO     [main2.py:92] Sample from training dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-12-07:10:01:21,363 INFO     [main2.py:95] Sample from validation dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-12-07:10:01:21,363 INFO     [main2.py:99] Tokenizing the Datasets\n",
      "2025-12-07:10:04:05,485 INFO     [main2.py:103] Tokenized the datasets\n",
      "2025-12-07:10:06:41,206 INFO     [main2.py:118] Dense model evaluation\n",
      "2025-12-07:10:06:41,206 INFO     [main2.py:119] Loading the model\n",
      "2025-12-07:10:06:41,206 INFO     [utilities.py:33] Loading the model\n",
      "config.json: 100%|█████████████████████████████| 601/601 [00:00<00:00, 5.81MB/s]\n",
      "model.safetensors.index.json: 23.9kB [00:00, 124MB/s]\n",
      "Downloading shards:   0%|                                 | 0/3 [00:00<?, ?it/s]\n",
      "model-00001-of-00003.safetensors:   0%|             | 0.00/4.95G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   1%|    | 62.0M/4.95G [00:02<02:59, 27.3MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:   4%|▏    | 196M/4.95G [00:02<01:00, 78.8MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  11%|▋     | 537M/4.95G [00:03<00:21, 206MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  31%|█▌   | 1.55G/4.95G [00:03<00:04, 769MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  37%|█▊   | 1.82G/4.95G [00:04<00:04, 715MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  41%|██   | 2.03G/4.95G [00:04<00:03, 800MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  45%|██▏  | 2.22G/4.95G [00:04<00:03, 799MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  49%|██▍  | 2.42G/4.95G [00:04<00:02, 889MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  53%|██▋  | 2.63G/4.95G [00:04<00:02, 962MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  57%|██▎ | 2.83G/4.95G [00:05<00:02, 1.03GB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  61%|██▍ | 3.03G/4.95G [00:05<00:01, 1.15GB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  65%|██▌ | 3.23G/4.95G [00:05<00:01, 1.22GB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  69%|██▊ | 3.43G/4.95G [00:05<00:01, 1.34GB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  73%|██▉ | 3.63G/4.95G [00:05<00:00, 1.46GB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  77%|███ | 3.81G/4.95G [00:05<00:00, 1.50GB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  81%|███▏| 4.01G/4.95G [00:05<00:00, 1.59GB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  85%|███▍| 4.22G/4.95G [00:05<00:00, 1.67GB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  89%|████▍| 4.42G/4.95G [00:06<00:00, 933MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  93%|████▋| 4.61G/4.95G [00:08<00:01, 281MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors:  96%|████▊| 4.75G/4.95G [00:08<00:00, 327MB/s]\u001b[A\n",
      "model-00001-of-00003.safetensors: 100%|█████| 4.95G/4.95G [00:08<00:00, 580MB/s]\u001b[A\n",
      "Downloading shards:  33%|████████▎                | 1/3 [00:09<00:18,  9.06s/it]\n",
      "model-00002-of-00003.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   0%|    | 724k/5.00G [00:01<2:35:08, 537kB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   4%|▏     | 202M/5.00G [00:01<00:36, 132MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:   8%|▍     | 404M/5.00G [00:02<00:22, 205MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  27%|█▎   | 1.34G/5.00G [00:02<00:04, 860MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  31%|█▌   | 1.55G/5.00G [00:02<00:03, 968MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  35%|█▋   | 1.75G/5.00G [00:03<00:03, 955MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  39%|█▉   | 1.95G/5.00G [00:03<00:03, 827MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  42%|██   | 2.09G/5.00G [00:03<00:03, 785MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  50%|█▉  | 2.49G/5.00G [00:03<00:02, 1.17GB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  54%|██▏ | 2.69G/5.00G [00:03<00:02, 1.07GB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  58%|██▎ | 2.89G/5.00G [00:04<00:01, 1.12GB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  62%|███  | 3.09G/5.00G [00:04<00:02, 938MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  64%|███▏ | 3.22G/5.00G [00:04<00:01, 913MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  67%|███▎ | 3.36G/5.00G [00:04<00:01, 961MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  70%|██▊ | 3.49G/5.00G [00:04<00:01, 1.00GB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  72%|██▉ | 3.62G/5.00G [00:04<00:01, 1.06GB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  75%|███ | 3.76G/5.00G [00:05<00:01, 1.07GB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  78%|███ | 3.89G/5.00G [00:05<00:01, 1.06GB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  81%|███▏| 4.03G/5.00G [00:05<00:00, 1.01GB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  83%|███▎| 4.16G/5.00G [00:05<00:00, 1.06GB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  87%|████▎| 4.33G/5.00G [00:05<00:00, 816MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  91%|████▌| 4.53G/5.00G [00:06<00:00, 673MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  93%|████▋| 4.66G/5.00G [00:06<00:00, 428MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  95%|████▋| 4.73G/5.00G [00:07<00:00, 371MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  97%|████▊| 4.87G/5.00G [00:07<00:00, 443MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors:  99%|████▉| 4.93G/5.00G [00:07<00:00, 339MB/s]\u001b[A\n",
      "model-00002-of-00003.safetensors: 100%|█████| 5.00G/5.00G [00:10<00:00, 490MB/s]\u001b[A\n",
      "Downloading shards:  67%|████████████████▋        | 2/3 [00:19<00:10, 10.06s/it]\n",
      "model-00003-of-00003.safetensors:   0%|             | 0.00/4.55G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   0%|    | 12.0M/4.55G [00:01<08:10, 9.24MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:   7%|▍     | 305M/4.55G [00:01<00:20, 209MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  15%|▉     | 679M/4.55G [00:02<00:10, 380MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  30%|█▍   | 1.35G/4.55G [00:02<00:03, 865MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  34%|█▋   | 1.55G/4.55G [00:02<00:03, 836MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  40%|█▉   | 1.82G/4.55G [00:02<00:02, 980MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  44%|██▏  | 2.02G/4.55G [00:03<00:02, 934MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  47%|██▎  | 2.15G/4.55G [00:03<00:02, 886MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  50%|██▌  | 2.29G/4.55G [00:03<00:02, 852MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  53%|██▋  | 2.42G/4.55G [00:03<00:02, 859MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  56%|██▊  | 2.55G/4.55G [00:03<00:02, 907MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  59%|██▉  | 2.69G/4.55G [00:03<00:02, 905MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  62%|███  | 2.82G/4.55G [00:04<00:01, 886MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  65%|███▏ | 2.94G/4.55G [00:04<00:02, 797MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  68%|███▍ | 3.07G/4.55G [00:04<00:01, 904MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  70%|███▌ | 3.21G/4.55G [00:04<00:01, 982MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  73%|███▋ | 3.34G/4.55G [00:04<00:01, 866MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  76%|███▊ | 3.47G/4.55G [00:04<00:01, 956MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  81%|███▏| 3.67G/4.55G [00:05<00:00, 1.07GB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  84%|███▎| 3.81G/4.55G [00:05<00:00, 1.13GB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  88%|███▌| 4.01G/4.55G [00:05<00:00, 1.20GB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  91%|████▌| 4.15G/4.55G [00:06<00:00, 489MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors:  94%|████▋| 4.28G/4.55G [00:06<00:00, 411MB/s]\u001b[A\n",
      "model-00003-of-00003.safetensors: 100%|█████| 4.55G/4.55G [00:07<00:00, 644MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 3/3 [00:27<00:00,  9.13s/it]\n",
      "2025-12-07:10:07:11,838 INFO     [modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:11<00:00,  3.83s/it]\n",
      "generation_config.json: 100%|███████████████████| 116/116 [00:00<00:00, 857kB/s]\n",
      "2025-12-07:10:07:24,306 INFO     [utilities.py:28] [Dense model] Full number of parameters = 7248023552\n",
      "2025-12-07:10:07:24,306 INFO     [utilities.py:29] [Dense model] Main model number of parameters = 6979584000\n",
      "Calculating perplexity: 100%|███████████████| 5926/5926 [39:37<00:00,  2.49it/s]\n",
      "2025-12-07:10:47:01,788 INFO     [main2.py:135] Perplexity (my_dataset): 2.71875\n"
     ]
    }
   ],
   "source": [
    "!python 2SSP/main2.py --model=mistralai/Mistral-7B-v0.3 --dense --evaluate_perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db736e4-4dc6-49b6-9844-8ab4df7c4eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-07:10:47:06,670 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-12-07:10:47:06,671 INFO     [main2.py:78] Loading the tokenizer\n",
      "tokenizer_config.json: 7.23kB [00:00, 33.9MB/s]\n",
      "vocab.json: 2.78MB [00:00, 8.65MB/s]\n",
      "merges.txt: 1.67MB [00:00, 8.58MB/s]\n",
      "tokenizer.json: 7.03MB [00:00, 17.0MB/s]\n",
      "2025-12-07:10:47:11,987 INFO     [main2.py:80] Loaded the tokenizer\n",
      "2025-12-07:10:47:11,987 INFO     [main2.py:83] Loading the Datasets\n",
      "2025-12-07:10:47:21,237 INFO     [main2.py:91] Loaded the Datasets\n",
      "2025-12-07:10:47:21,237 INFO     [main2.py:92] Sample from training dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-12-07:10:47:21,237 INFO     [main2.py:95] Sample from validation dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-12-07:10:47:21,237 INFO     [main2.py:99] Tokenizing the Datasets\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (299078 > 131072). Running this sequence through the model will result in indexing errors\n",
      "2025-12-07:10:48:18,501 INFO     [main2.py:103] Tokenized the datasets\n",
      "2025-12-07:10:49:10,564 INFO     [main2.py:118] Dense model evaluation\n",
      "2025-12-07:10:49:10,564 INFO     [main2.py:119] Loading the model\n",
      "2025-12-07:10:49:10,564 INFO     [utilities.py:33] Loading the model\n",
      "config.json: 100%|█████████████████████████████| 686/686 [00:00<00:00, 4.66MB/s]\n",
      "model.safetensors.index.json: 27.8kB [00:00, 146MB/s]\n",
      "Downloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]\n",
      "model-00001-of-00004.safetensors:   0%|             | 0.00/3.95G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   0%| | 14.8k/3.95G [00:02<191:15:34, 5.73kB/s\u001b[A\n",
      "model-00001-of-00004.safetensors:   0%|    | 11.0M/3.95G [00:02<11:30, 5.69MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 41.4M/3.95G [00:02<02:28, 26.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|    | 63.6M/3.95G [00:02<01:29, 43.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏     | 127M/3.95G [00:03<00:34, 109MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏     | 163M/3.95G [00:03<00:26, 142MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▎     | 198M/3.95G [00:03<00:21, 175MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎     | 231M/3.95G [00:03<00:18, 196MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▌     | 366M/3.95G [00:03<00:08, 428MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▋     | 455M/3.95G [00:03<00:06, 528MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▉     | 608M/3.95G [00:03<00:04, 769MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|█     | 705M/3.95G [00:03<00:04, 671MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|█▏    | 817M/3.95G [00:04<00:05, 582MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|█▎    | 888M/3.95G [00:04<00:11, 272MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|█▍    | 986M/3.95G [00:04<00:08, 329MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█▎   | 1.06G/3.95G [00:05<00:08, 331MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▍   | 1.16G/3.95G [00:05<00:08, 343MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▌   | 1.23G/3.95G [00:05<00:08, 330MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▉   | 1.54G/3.95G [00:05<00:03, 679MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|██▏  | 1.68G/3.95G [00:06<00:03, 670MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|██▎  | 1.82G/3.95G [00:06<00:03, 646MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|██▍  | 1.89G/3.95G [00:06<00:03, 607MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██▌  | 2.03G/3.95G [00:06<00:02, 641MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▋  | 2.13G/3.95G [00:06<00:03, 603MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▉  | 2.27G/3.95G [00:07<00:02, 659MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|███  | 2.40G/3.95G [00:07<00:02, 667MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|███▏ | 2.54G/3.95G [00:07<00:02, 662MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|███▍ | 2.74G/3.95G [00:07<00:01, 855MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|███▋ | 2.87G/3.95G [00:07<00:01, 928MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███▊ | 3.01G/3.95G [00:07<00:00, 982MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▉ | 3.14G/3.95G [00:07<00:00, 914MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|████▏| 3.27G/3.95G [00:08<00:00, 991MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 3.48G/3.95G [00:08<00:00, 1.09GB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 3.61G/3.95G [00:08<00:00, 1.13GB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 3.74G/3.95G [00:08<00:00, 1.04GB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|█████| 3.95G/3.95G [00:08<00:00, 455MB/s]\u001b[A\n",
      "Downloading shards:  25%|██████▎                  | 1/4 [00:09<00:27,  9.21s/it]\n",
      "model-00002-of-00004.safetensors:   0%|             | 0.00/3.86G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   0%|    | 3.20M/3.86G [00:01<24:16, 2.65MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   6%|▎     | 220M/3.86G [00:01<00:24, 149MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|█▎    | 871M/3.86G [00:01<00:04, 686MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▍   | 1.13G/3.86G [00:02<00:04, 608MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▊   | 1.37G/3.86G [00:02<00:03, 707MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|██   | 1.60G/3.86G [00:02<00:02, 868MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|██▎  | 1.81G/3.86G [00:03<00:02, 899MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██▌  | 2.01G/3.86G [00:03<00:02, 825MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▊  | 2.14G/3.86G [00:03<00:01, 877MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|██▉  | 2.31G/3.86G [00:03<00:01, 804MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|███▏ | 2.44G/3.86G [00:03<00:02, 712MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|███▎ | 2.60G/3.86G [00:04<00:01, 804MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|███▌ | 2.73G/3.86G [00:04<00:01, 894MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|███▋ | 2.86G/3.86G [00:04<00:01, 895MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▉ | 3.05G/3.86G [00:04<00:00, 999MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|███▎| 3.18G/3.86G [00:04<00:00, 1.06GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 3.32G/3.86G [00:04<00:00, 1.12GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 3.45G/3.86G [00:04<00:00, 1.15GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▊| 3.63G/3.86G [00:04<00:00, 1.22GB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|█████| 3.86G/3.86G [00:05<00:00, 721MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████▌            | 2/4 [00:15<00:14,  7.26s/it]\n",
      "model-00003-of-00004.safetensors:   0%|             | 0.00/3.86G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   0%|    | 8.39M/3.86G [00:01<10:42, 6.00MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▋     | 475M/3.86G [00:02<00:11, 289MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|█▎    | 873M/3.86G [00:02<00:06, 459MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▋   | 1.29G/3.86G [00:02<00:03, 688MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47%|█▉  | 1.81G/3.86G [00:02<00:01, 1.12GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.08G/3.86G [00:02<00:01, 1.22GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 2.35G/3.86G [00:03<00:01, 1.27GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|██▋ | 2.56G/3.86G [00:03<00:00, 1.37GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|██▊ | 2.76G/3.86G [00:03<00:00, 1.42GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███ | 2.98G/3.86G [00:03<00:00, 1.48GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|███▎| 3.17G/3.86G [00:03<00:00, 1.53GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 3.37G/3.86G [00:03<00:00, 1.62GB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 3.57G/3.86G [00:11<00:03, 89.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100%|█████| 3.86G/3.86G [00:11<00:00, 333MB/s]\u001b[A\n",
      "Downloading shards:  75%|██████████████████▊      | 3/4 [00:27<00:09,  9.49s/it]\n",
      "model-00004-of-00004.safetensors:   0%|             | 0.00/3.56G [00:00<?, ?B/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   2%|    | 67.1M/3.56G [00:01<01:01, 56.9MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   9%|▌     | 335M/3.56G [00:01<00:09, 327MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  13%|▊     | 461M/3.56G [00:01<00:07, 433MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  16%|▉     | 581M/3.56G [00:01<00:05, 534MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  22%|█▎    | 769M/3.56G [00:01<00:03, 727MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  26%|█▌    | 913M/3.56G [00:02<00:05, 477MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  45%|█▊  | 1.61G/3.56G [00:02<00:01, 1.11GB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  50%|██  | 1.79G/3.56G [00:02<00:01, 1.18GB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  55%|██▋  | 1.95G/3.56G [00:02<00:01, 859MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  59%|██▉  | 2.08G/3.56G [00:03<00:01, 835MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  62%|███  | 2.22G/3.56G [00:03<00:01, 756MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  66%|███▎ | 2.35G/3.56G [00:03<00:01, 775MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  70%|███▍ | 2.49G/3.56G [00:03<00:01, 750MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  74%|███▋ | 2.64G/3.56G [00:03<00:01, 789MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  78%|███▉ | 2.77G/3.56G [00:04<00:00, 814MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  82%|████ | 2.90G/3.56G [00:04<00:00, 753MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  85%|████▏| 3.02G/3.56G [00:04<00:00, 702MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  89%|████▍| 3.15G/3.56G [00:04<00:00, 742MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  92%|████▌| 3.29G/3.56G [00:04<00:00, 837MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  96%|████▊| 3.42G/3.56G [00:04<00:00, 924MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors: 100%|█████| 3.56G/3.56G [00:04<00:00, 719MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [00:32<00:00,  8.18s/it]\n",
      "2025-12-07:10:49:46,498 INFO     [modeling.py:1004] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.38s/it]\n",
      "generation_config.json: 100%|██████████████████| 138/138 [00:00<00:00, 1.12MB/s]\n",
      "2025-12-07:10:49:52,923 INFO     [utilities.py:28] [Dense model] Full number of parameters = 7615616512\n",
      "2025-12-07:10:49:52,923 INFO     [utilities.py:29] [Dense model] Main model number of parameters = 6525618176\n",
      "Calculating perplexity: 100%|███████████████| 5553/5553 [33:52<00:00,  2.73it/s]\n",
      "2025-12-07:11:23:46,089 INFO     [main2.py:135] Perplexity (my_dataset): 3.109375\n"
     ]
    }
   ],
   "source": [
    "!python 2SSP/main2.py --model=Qwen/Qwen2.5-7B --dense --evaluate_perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131a2aa2-343a-4a88-b498-73c56f195d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-07:20:38:24,940 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-12-07:20:38:24,940 INFO     [main2.py:78] Loading the tokenizer\n",
      "2025-12-07:20:38:25,387 INFO     [main2.py:80] Loaded the tokenizer\n",
      "2025-12-07:20:38:25,387 INFO     [main2.py:83] Loading the Datasets\n",
      "2025-12-07:20:38:34,328 INFO     [main2.py:91] Loaded the Datasets\n",
      "2025-12-07:20:38:34,329 INFO     [main2.py:92] Sample from training dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-12-07:20:38:34,329 INFO     [main2.py:95] Sample from validation dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-12-07:20:38:34,329 INFO     [main2.py:99] Tokenizing the Datasets\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (299078 > 131072). Running this sequence through the model will result in indexing errors\n",
      "2025-12-07:20:39:31,357 INFO     [main2.py:103] Tokenized the datasets\n",
      "2025-12-07:20:40:22,992 INFO     [main2.py:149] Loading the model\n",
      "2025-12-07:20:40:22,992 INFO     [utilities.py:33] Loading the model\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.30s/it]\n",
      "2025-12-07:20:40:29,931 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-12-07:20:40:29,931 INFO     [main2.py:185] Pruning rate 25.0 (equivalent of 7.0 blocks)\n",
      "2025-12-07:20:40:29,932 INFO     [pruning.py:248] Pruning 0 attention submodules\n",
      "First stage: 100%|██████████████████████████████| 32/32 [01:08<00:00,  2.15s/it]\n",
      "Second stage: 0it [00:00, ?it/s]\n",
      "2025-12-07:20:41:39,996 INFO     [main2.py:206] Pruning Time: 70.06504654884338 s\n",
      "2025-12-07:20:41:39,998 INFO     [utilities.py:28] [Pruned model] Full number of parameters = 5984194048\n",
      "2025-12-07:20:41:39,998 INFO     [utilities.py:29] [Pruned model] Main model number of parameters = 4894195712\n",
      "Calculating perplexity: 100%|███████████████| 5553/5553 [46:10<00:00,  2.00it/s]\n",
      "2025-12-07:21:27:50,451 INFO     [main2.py:224] Perplexity (my_dataset): 3.234375\n",
      "2025-12-07:21:27:50,658 INFO     [utilities.py:33] Loading the model\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:04<00:00,  1.23s/it]\n",
      "2025-12-07:21:27:56,423 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-12-07:21:27:56,424 INFO     [main2.py:185] Pruning rate 37.5 (equivalent of 10.5 blocks)\n",
      "2025-12-07:21:27:56,425 INFO     [pruning.py:248] Pruning 0 attention submodules\n",
      "First stage: 100%|██████████████████████████████| 32/32 [01:07<00:00,  2.11s/it]\n",
      "Second stage: 0it [00:00, ?it/s]\n",
      "2025-12-07:21:29:04,644 INFO     [main2.py:206] Pruning Time: 68.22060656547546 s\n",
      "2025-12-07:21:29:04,646 INFO     [utilities.py:28] [Pruned model] Full number of parameters = 5168633344\n",
      "2025-12-07:21:29:04,646 INFO     [utilities.py:29] [Pruned model] Main model number of parameters = 4078635008\n",
      "Calculating perplexity: 100%|███████████████| 5553/5553 [16:25<00:00,  5.63it/s]\n",
      "2025-12-07:21:45:30,621 INFO     [main2.py:224] Perplexity (my_dataset): 3.4375\n",
      "2025-12-07:21:45:31,050 INFO     [utilities.py:33] Loading the model\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:04<00:00,  1.21s/it]\n",
      "2025-12-07:21:45:36,740 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-12-07:21:45:36,740 INFO     [main2.py:185] Pruning rate 50.0 (equivalent of 14.0 blocks)\n",
      "2025-12-07:21:45:36,741 INFO     [pruning.py:248] Pruning 1 attention submodules\n",
      "First stage: 100%|██████████████████████████████| 32/32 [01:07<00:00,  2.11s/it]\n",
      "Second stage: 100%|███████████████████████████████| 1/1 [00:05<00:00,  5.50s/it]\n",
      "2025-12-07:21:46:50,652 INFO     [main2.py:206] Pruning Time: 73.91235089302063 s\n",
      "2025-12-07:21:46:50,654 INFO     [utilities.py:28] [Pruned model] Full number of parameters = 4352910336\n",
      "2025-12-07:21:46:50,654 INFO     [utilities.py:29] [Pruned model] Main model number of parameters = 3262912000\n",
      "Calculating perplexity: 100%|███████████████| 5553/5553 [18:31<00:00,  5.00it/s]\n",
      "2025-12-07:22:05:22,546 INFO     [main2.py:224] Perplexity (my_dataset): 3.71875\n",
      "2025-12-07:22:05:22,698 INFO     [utilities.py:33] Loading the model\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "!python 2SSP/main2.py --model=Qwen/Qwen2.5-7B --pruning_method=2ssp --sparsity_rate=-2 --evaluate_perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a0aad2-71d7-4fe0-baeb-54026a59ebc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-08:23:51:02,434 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-12-08:23:51:02,434 INFO     [main2.py:78] Loading the tokenizer\n",
      "2025-12-08:23:51:02,822 INFO     [main2.py:80] Loaded the tokenizer\n",
      "2025-12-08:23:51:02,822 INFO     [main2.py:83] Loading the Datasets\n",
      "2025-12-08:23:51:12,424 INFO     [main2.py:91] Loaded the Datasets\n",
      "2025-12-08:23:51:12,424 INFO     [main2.py:92] Sample from training dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-12-08:23:51:12,424 INFO     [main2.py:95] Sample from validation dataset:\n",
      "{'text': 'Do you have any close family members who suffer from allergies (any type), hay fever or eczema?: yes Do you have any family members who have asthma?: yes Do you have asthma or have you ever had to use a bronchodilator in the past?: yes Is your nose or the back of your throat itchy?: yes Do you have nasal congestion or a clear runny nose?: yes Do you have a cough?: yes Have you traveled out of the country in the last 4 weeks?: N Do you live in in a big city?: yes'}\n",
      "2025-12-08:23:51:12,424 INFO     [main2.py:99] Tokenizing the Datasets\n",
      "2025-12-08:23:54:10,724 INFO     [main2.py:103] Tokenized the datasets\n",
      "2025-12-08:23:56:58,290 INFO     [main2.py:149] Loading the model\n",
      "2025-12-08:23:56:58,290 INFO     [utilities.py:33] Loading the model\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:10<00:00,  3.42s/it]\n",
      "2025-12-08:23:57:10,439 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-12-08:23:57:10,439 INFO     [main2.py:185] Pruning rate 25.0 (equivalent of 8.0 blocks)\n",
      "2025-12-08:23:57:10,440 INFO     [pruning.py:248] Pruning 1 attention submodules\n",
      "First stage: 100%|██████████████████████████████| 32/32 [01:12<00:00,  2.27s/it]\n",
      "Second stage: 100%|███████████████████████████████| 1/1 [00:14<00:00, 14.05s/it]\n",
      "2025-12-08:23:58:38,491 INFO     [main2.py:206] Pruning Time: 88.05277562141418 s\n",
      "2025-12-08:23:58:38,493 INFO     [utilities.py:28] [Pruned model] Full number of parameters = 5503062016\n",
      "2025-12-08:23:58:38,493 INFO     [utilities.py:29] [Pruned model] Main model number of parameters = 5234622464\n",
      "Calculating perplexity: 100%|███████████████| 5926/5926 [32:10<00:00,  3.07it/s]\n",
      "2025-12-09:00:30:49,378 INFO     [main2.py:224] Perplexity (my_dataset): 3.0390625\n",
      "2025-12-09:00:30:49,726 INFO     [utilities.py:33] Loading the model\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:06<00:00,  2.26s/it]\n",
      "2025-12-09:00:30:57,381 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-12-09:00:30:57,381 INFO     [main2.py:185] Pruning rate 37.5 (equivalent of 12.0 blocks)\n",
      "2025-12-09:00:30:57,383 INFO     [pruning.py:248] Pruning 2 attention submodules\n",
      "First stage: 100%|██████████████████████████████| 32/32 [01:06<00:00,  2.06s/it]\n",
      "Second stage: 100%|███████████████████████████████| 2/2 [00:16<00:00,  8.13s/it]\n",
      "2025-12-09:00:32:20,122 INFO     [main2.py:206] Pruning Time: 82.74070692062378 s\n",
      "2025-12-09:00:32:20,124 INFO     [utilities.py:28] [Pruned model] Full number of parameters = 4630646784\n",
      "2025-12-09:00:32:20,124 INFO     [utilities.py:29] [Pruned model] Main model number of parameters = 4362207232\n",
      "Calculating perplexity: 100%|███████████████| 5926/5926 [25:29<00:00,  3.87it/s]\n",
      "2025-12-09:00:57:50,178 INFO     [main2.py:224] Perplexity (my_dataset): 3.166015625\n",
      "2025-12-09:00:57:50,347 INFO     [utilities.py:33] Loading the model\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:06<00:00,  2.27s/it]\n",
      "2025-12-09:00:57:57,925 INFO     [utilities.py:19] Seed for reproducibility: 0\n",
      "2025-12-09:00:57:57,925 INFO     [main2.py:185] Pruning rate 50.0 (equivalent of 16.0 blocks)\n",
      "2025-12-09:00:57:57,926 INFO     [pruning.py:248] Pruning 5 attention submodules\n",
      "First stage: 100%|██████████████████████████████| 32/32 [01:07<00:00,  2.10s/it]\n",
      "Second stage: 100%|███████████████████████████████| 5/5 [00:20<00:00,  4.10s/it]\n",
      "2025-12-09:00:59:26,020 INFO     [main2.py:206] Pruning Time: 88.09491610527039 s\n",
      "2025-12-09:00:59:26,022 INFO     [utilities.py:28] [Pruned model] Full number of parameters = 3758100480\n",
      "2025-12-09:00:59:26,022 INFO     [utilities.py:29] [Pruned model] Main model number of parameters = 3489660928\n",
      "Calculating perplexity: 100%|███████████████| 5926/5926 [13:15<00:00,  7.45it/s]\n",
      "2025-12-09:01:12:42,059 INFO     [main2.py:224] Perplexity (my_dataset): 3.517578125\n",
      "2025-12-09:01:12:42,216 INFO     [utilities.py:33] Loading the model\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:06<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "!python 2SSP/main2.py --model=mistralai/Mistral-7B-v0.3 --pruning_method=2ssp --sparsity_rate=-2 --evaluate_perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1dccc4-84bc-4e09-aa81-2468446013ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Torch)",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
